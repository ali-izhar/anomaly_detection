# src/models2/train_config.yaml

data:
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  seq_len: 200
  n_nodes: 50
  n_features: 6
  batch_size: 128
  m_horizon: 10

model:
  gnn:
    hidden_dim: 256
    num_layers: 3
    activation: "relu"
  temporal:
    type: "LSTM"
    hidden_dim: 512
    num_layers: 2
    dropout: 0.3
  forecast:
    m_horizon: 10

training:
  epochs: 2
  learning_rate: 0.001
  weight_decay: 0.0001
  early_stopping_patience: 15
  grad_clip: 0.5
  gpu_memory_fraction: 0.99
  optimizer:
    type: "AdamW"
    betas: [0.9, 0.999]
    eps: 1e-8
  scheduler:
    type: "ReduceLROnPlateau"
    mode: "min"
    factor: 0.5
    patience: 5
    min_lr: 1e-6
    cooldown: 2

hardware:
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  mixed_precision: true
  cudnn_benchmark: true
  memory_efficient_attention: true
  gradient_checkpointing: true
  optimize_memory_usage: true
  attention:
    num_heads: 4
    chunk_size: 128

device: "cuda"
