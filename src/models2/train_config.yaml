# src/models2/train_config.yaml

data:
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  seq_len: 200
  n_nodes: 50
  n_features: 6
  batch_size: 128
  m_horizon: 10

model:
  gnn:
    hidden_dim: 128
    num_layers: 2
    activation: "gelu"
  temporal:
    type: "LSTM"
    hidden_dim: 256
    num_layers: 2
    dropout: 0.2
  forecast:
    m_horizon: 10

training:
  epochs: 10
  learning_rate: 0.0001
  weight_decay: 0.0001
  grad_accum_steps: 4
  warmup_steps: 100
  ema_decay: 0.999
  early_stopping_patience: 5
  grad_clip: 0.1
  gpu_memory_fraction: 0.99
  optimizer:
    type: "AdamW"
    betas: [0.9, 0.999]
    eps: 1e-8
  scheduler:
    type: "ReduceLROnPlateau"
    mode: "min"
    factor: 0.2
    patience: 2
    min_lr: 1e-5
    cooldown: 0

hardware:
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  mixed_precision: true
  cudnn_benchmark: true
  memory_efficient_attention: true
  gradient_checkpointing: true
  optimize_memory_usage: true
  attention:
    num_heads: 2
    chunk_size: 128

device: "cuda"
