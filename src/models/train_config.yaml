# Training Configuration
paths:
  data_dir: "dataset"
  output_dir: "outputs"

# Random seed for reproducibility
seed: 42

data:
  # Sequence parameters
  window_size: 20
  stride: 5
  forecast_horizon: 10
  batch_size: 128  # Increased for better GPU utilization
  
  # Feature selection
  use_centrality: true
  use_spectral: true
  enable_augmentation: false
  noise_level: 0.01
  
  # Dataset constants
  num_nodes: 100
  max_seq_length: 200
  min_seq_length: 161
  num_change_points: 2
  
  # Feature dimensions
  svd_dim: 2
  lsvd_dim: 16

model:
  # Model architecture
  hidden_dim: 256        # Increased for better GPU utilization
  num_layers: 3         # More layers
  dropout: 0.2         # Regularization
  
  # Feature processing
  use_node_features: true
  use_edge_features: true
  combine_features: "concat"
  
  # GNN parameters
  gnn_type: "gcn"
  attention_heads: 8    # Increased attention heads
  
  # LSTM parameters
  lstm_layers: 3        # Increased LSTM layers
  bidirectional: true   # Enable bidirectional
  
  # Training parameters
  learning_rate: 0.001
  weight_decay: 0.0001  # L2 regularization
  clip_grad_norm: 1.0   # Increased for stability

training:
  epochs: 100           # Maximum epochs
  patience: 15         # Early stopping patience
  val_interval: 1      # Validate every epoch
  save_interval: 5     # Save checkpoints every N epochs
  
  # Loss function weights
  loss_weights:
    degree: 1.0
    betweenness: 1.0
    closeness: 1.0
    eigenvector: 1.0
    svd: 0.5           # Lower weight for embeddings
    lsvd: 0.5
    
  # Learning rate schedule
  lr_schedule:
    factor: 0.5        # Multiply LR by this when plateauing
    patience: 5        # Epochs to wait before reducing LR
    min_lr: 0.000001   # Minimum learning rate
    
  # Logging and monitoring
  log_interval: 50     # Log every N batches
  tensorboard: true    # Enable tensorboard logging

# Hardware optimization for RTX 4090
hardware:
  num_workers: 8       # DataLoader workers
  pin_memory: true     # Pin memory for faster GPU transfer
  prefetch_factor: 4   # Number of batches to prefetch
  persistent_workers: true  # Keep workers alive between epochs
  cuda_settings:
    allow_tf32: true
    benchmark: true
    memory_allocation: "optimal"
    max_split_size_mb: 512
